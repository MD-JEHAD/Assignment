{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720a75bb-f22a-4e5a-aaa5-ba412cd7e1e5",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123b5b6-7a16-4285-9e59-ae477a473990",
   "metadata": {},
   "source": [
    "Assumptions of ANOVA:\n",
    "\n",
    "1. Independence: The observations within each group should be independent of each other. This means that the data points within a group should not be influenced by or related to each other. Violations of this assumption can occur when there is a correlation between observations within a group, leading to potential bias in the results.\n",
    "\n",
    "2. Normality: The residuals (the differences between observed values and predicted values) within each group should follow a normal distribution. This assumption is critical, especially for smaller sample sizes, as ANOVA becomes more robust with larger sample sizes. Departures from normality can lead to inaccurate p-values and incorrect conclusions about the significance of group differences.\n",
    "\n",
    "3. Homogeneity of Variance (Homoscedasticity): The variances of the residuals within each group should be roughly equal. In other words, the spread of data points should be consistent across all groups. If there's a significant difference in the variability of the groups, the results of ANOVA can be compromised. This can be tested using techniques like Levene's test or the Bartlett test.\n",
    "\n",
    "4. Homogeneity of Regression Slopes (if applicable): If you're performing an ANOVA with a regression component (e.g., ANCOVA), the relationship between the dependent variable and the covariate should be similar across all groups. If this assumption is violated, it's called the \"homogeneity of regression slopes\" assumption.\n",
    "\n",
    "5. Equal Group Sizes (for one-way ANOVA): In a one-way ANOVA, where you're comparing means across different groups, having roughly equal group sizes is preferred. Unequal group sizes can affect the statistical power and precision of the analysis.\n",
    "\n",
    "6. Random Sampling: The data should be collected through a random sampling process, ensuring that the observations are representative of the broader population. This assumption is important for making valid inferences about the population.\n",
    "\n",
    "Examples of Violations and Impacts:\n",
    "\n",
    "    Non-Normality: If the residuals within each group do not follow a normal distribution, the p-values and confidence intervals obtained from ANOVA might be inaccurate. This can lead to incorrect conclusions about group differences.\n",
    "\n",
    "    Heteroscedasticity: Unequal variances across groups can lead to inflated Type I error rates (incorrectly rejecting the null hypothesis) and reduced statistical power. The F-test in ANOVA assumes equal variances, so violations can compromise the validity of the results.\n",
    "\n",
    "    Independence Violation: Correlation among observations within groups can lead to biased results. For example, in a study involving repeated measurements on the same subjects, violating the assumption of independence can lead to inflated significance levels.\n",
    "\n",
    "    Unequal Group Sizes (for one-way ANOVA): Unequal group sizes can affect the precision of the results. Smaller groups might have less statistical power to detect true differences between means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135d10d-ebf9-4c00-8722-89e24b742bc3",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c50bc-51a1-4e69-88ee-d1b7524ffb6d",
   "metadata": {},
   "source": [
    "There are 3 types of anova\n",
    "1. One-Way ANOVA:\n",
    "\n",
    "Example: Imagine you are studying the effect of different teaching methods on student test scores. You have three groups: Group A with traditional teaching, Group B with online teaching, and Group C with interactive teaching. One-way ANOVA can be used to determine if there are significant differences in test scores among these teaching methods.\n",
    "\n",
    "2. Two-Way ANOVA:\n",
    "\n",
    "Example: Suppose you are investigating the effects of both teaching method (traditional, online, interactive) and gender (male, female) on student test scores. Two-way ANOVA can help you determine if there are main effects of teaching method and gender, as well as whether the interaction between these two factors has a significant impact on test scores.\n",
    "\n",
    "3. Repeated Measures ANOVA:\n",
    "\n",
    "Example: You are conducting a study to examine the effect of a new drug on blood pressure. You measure the blood pressure of each participant before taking the drug, after one week of taking the drug, and after two weeks of taking the drug. Repeated measures ANOVA can help you determine if there are significant differences in blood pressure across the different time points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f418074-b18a-4dc1-9fe4-43f780173254",
   "metadata": {},
   "source": [
    "# Q3\n",
    "\n",
    "The partitioning of variance in ANOVA refers to the process of breaking down the total variability observed in a dataset into different components that can be attributed to various sources of variation. This concept is fundamental to understanding the sources of variation and their contributions to the overall variability in the data. ANOVA achieves this partitioning by decomposing the total sum of squares (SS) into several components, including the sum of squares between groups (SSB), the sum of squares within groups (SSW), and sometimes the sum of squares due to interactions (if applicable).\n",
    "\n",
    "The partitioning of variance is important for several reasons:\n",
    "\n",
    "1. Source of Variation: By breaking down the total variance into different components, ANOVA helps you identify which sources of variation contribute significantly to the differences between groups. This allows you to attribute variability to specific factors or treatments under investigation.\n",
    "\n",
    "2. Hypothesis Testing: ANOVA uses the partitioning of variance to perform hypothesis tests that compare the variation between groups to the variation within groups. This is essential for determining whether the observed differences in means among groups are statistically significant or simply due to random chance.\n",
    "\n",
    "3. F-Statistic Calculation: The partitioning of variance is used to calculate the F-statistic, which is the ratio of the between-group variation to the within-group variation. The F-statistic is then compared to a critical value to determine if the group means are significantly different from each other.\n",
    "\n",
    "4. Interpretation of Results: Understanding the partitioning of variance allows researchers to interpret ANOVA results more effectively. You can determine the proportion of variance explained by the independent variable (or factors) and compare it to the proportion that remains unexplained.\n",
    "\n",
    "5. Design and Analysis Improvement: If the partitioning of variance reveals that a significant proportion of the total variance is due to a particular factor or interaction, it suggests that this factor has a substantial effect on the dependent variable. This insight can guide further research, experimental design adjustments, or exploration of potential covariates.\n",
    "\n",
    "6. Assumptions Checking: The partitioning of variance highlights the role of variance components, helping researchers identify potential violations of ANOVA assumptions, such as homoscedasticity or normality. If the partitioned variance components are not as expected, it may signal the need for further investigation or alternative analysis methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc8137-0e13-4aeb-9b47-f9ffa526c0fd",
   "metadata": {},
   "source": [
    "# Q4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b184af90-2fbe-4171-8d54-c5f993269795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 2103.6000000000004\n",
      "Explained Sum of Squares (SSE): 1909.2000000000003\n",
      "Residual Sum of Squares (SSR): 194.4000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "group1 = np.array([15, 18, 20, 22, 25])\n",
    "group2 = np.array([28, 30, 32, 35, 38])\n",
    "group3 = np.array([42, 45, 48, 50, 53])\n",
    "all_data = np.concatenate([group1, group2, group3])\n",
    "overall_mean = np.mean(all_data)\n",
    "sst = np.sum((all_data - overall_mean) ** 2)\n",
    "group_means = np.array([np.mean(group1), np.mean(group2), np.mean(group3)])\n",
    "sse = np.sum((group_means - overall_mean) ** 2) * len(group1)\n",
    "ssr = sst - sse\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef1299-1a89-4547-be6d-f483e4ea8f2a",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22ddee6-d092-4d16-ad8d-9ecbc228742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect of Factor A: 150.51851851851856\n",
      "Main Effect of Factor B: 20.074074074074066\n",
      "Interaction Effect: 5139.222222222223\n",
      "Error: -4794.925925925926\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "data = np.array([\n",
    "    [12, 15, 18],\n",
    "    [20, 25, 28],\n",
    "    [30, 32, 35]\n",
    "])\n",
    "mean_factor_a = np.mean(data, axis=1)\n",
    "mean_factor_b = np.mean(data, axis=0)\n",
    "overall_mean = np.mean(data)\n",
    "main_effect_a = np.sum((mean_factor_a - overall_mean) ** 2)\n",
    "main_effect_b = np.sum((mean_factor_b - overall_mean) ** 2)\n",
    "interaction_effect = np.sum((data - mean_factor_a[:, np.newaxis] - mean_factor_b) ** 2)\n",
    "sst = np.sum((data - overall_mean) ** 2)\n",
    "ss_main_effects = main_effect_a + main_effect_b\n",
    "ss_interaction_effect = interaction_effect\n",
    "ss_error = sst - ss_main_effects - ss_interaction_effect\n",
    "\n",
    "print(\"Main Effect of Factor A:\", main_effect_a)\n",
    "print(\"Main Effect of Factor B:\", main_effect_b)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n",
    "print(\"Error:\", ss_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e95598-90f7-431d-a69e-b5fdbde05934",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f66414-5120-44ba-990e-ad50b3474a6b",
   "metadata": {},
   "source": [
    "In your case, you obtained an F-statistic of 5.23 and a p-value of 0.02. Here's how you can interpret these results:\n",
    "\n",
    "    F-Statistic: The F-statistic of 5.23 indicates the ratio of variability between group means to the variability within groups. A larger F-statistic suggests that the variation between the group means is relatively larger compared to the variation within the groups.\n",
    "\n",
    "    p-value: The p-value of 0.02 indicates that the probability of observing an F-statistic as extreme as 5.23 (or more extreme) under the assumption of no real differences between the group means is 0.02. In other words, if the null hypothesis were true (i.e., if the group means were all equal), you would only expect to see an F-statistic as extreme as 5.23 about 2% of the time.\n",
    "\n",
    "    Conclusion: With a p-value of 0.02, you would typically use a predetermined significance level (often denoted as α, commonly set to 0.05) to make a decision. If the p-value is less than or equal to α, you reject the null hypothesis. In this case, since the p-value (0.02) is less than α (0.05), you would conclude that there is sufficient evidence to reject the null hypothesis.\n",
    "\n",
    "    Interpretation: Based on these results, you can interpret that there are likely significant differences between the groups. In other words, at least one group mean is different from the others. However, the ANOVA itself doesn't tell you which specific groups have different means. If you reject the null hypothesis, you might proceed with post hoc tests (such as Tukey's HSD or Bonferroni corrections) to determine which groups differ significantly from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ede8d-c1e3-4f63-91b3-8c69288d42df",
   "metadata": {},
   "source": [
    "# Q7 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd21d7-7219-4a63-a44a-4e726b96d252",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is essential to ensure the validity and reliability of your results. Missing data can occur for various reasons, such as participant dropout, measurement errors, or other sources of nonresponse. There are several methods to handle missing data in a repeated measures ANOVA, each with its own implications:\n",
    "\n",
    "    Listwise Deletion (Complete Case Analysis): This method involves removing any participant with missing data from the analysis. While it's straightforward, it can lead to a reduction in sample size, loss of statistical power, and potential bias if the missing data is not random. The remaining participants may not be representative of the original sample.\n",
    "\n",
    "    Mean Imputation: Missing values are replaced with the mean value of the available data for that variable. While simple to implement, mean imputation can underestimate standard errors, distort relationships, and potentially result in biased estimates if the data is not missing at random.\n",
    "\n",
    "    Last Observation Carried Forward (LOCF): Missing values are replaced with the last observed value for that participant. This method can lead to incorrect estimates of group differences if the data is not missing completely at random and if the pattern of missingness is related to the dependent variable.\n",
    "\n",
    "    Linear Interpolation: Missing values are estimated based on the trend of the observed data points before and after the missing value. This method assumes a linear relationship and may not work well for non-linear data.\n",
    "\n",
    "    Multiple Imputation: This involves creating multiple plausible imputed datasets and analyzing each separately. The results are then combined to produce valid estimates and appropriate standard errors. Multiple imputation provides a robust approach if assumptions about the missing data mechanism are met, but it requires more complex analyses.\n",
    "\n",
    "    Model-Based Methods: Advanced methods, such as mixed-effects models, can be used to simultaneously model the repeated measures and handle missing data. These methods take advantage of the available data and account for the within-subject correlations.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data include:\n",
    "\n",
    "    Bias: Methods like mean imputation or LOCF can introduce bias if the missing data mechanism is not random.\n",
    "    Loss of Power: Removing participants with missing data or imputing missing values with crude methods can reduce the effective sample size and lower statistical power.\n",
    "    Invalid Inferences: Incorrectly handling missing data can lead to incorrect conclusions and invalid inferences about group differences and relationships.\n",
    "    Inaccurate Estimates: Some methods may underestimate or overestimate variability and relationships in the data.\n",
    "    Assumption Violations: Some methods may assume that the missing data mechanism is ignorable or missing at random, which might not hold in real-world situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0785d1c0-b745-4080-a372-c277c42e1f04",
   "metadata": {},
   "source": [
    "# Q8\n",
    "Some common post-hoc tests include:\n",
    "\n",
    "    Tukey's Honestly Significant Difference (HSD): Tukey's HSD is a conservative post-hoc test that controls the familywise error rate. It's used when you want to compare all possible pairs of group means. Tukey's HSD is suitable when you have more than three groups and you're interested in comprehensively assessing differences among them.\n",
    "\n",
    "    Bonferroni Correction: This method involves adjusting the significance level (alpha) for each comparison to control the familywise error rate. It's a more stringent correction that reduces the likelihood of Type I errors. Bonferroni correction is used when you want to control for the overall experiment-wise error rate but can result in increased Type II errors (false negatives).\n",
    "\n",
    "    Sidak Correction: Similar to the Bonferroni correction, the Sidak correction adjusts the significance level for each comparison. However, it's a slightly less conservative correction, providing a balance between controlling Type I errors and maintaining power.\n",
    "\n",
    "    Dunn's Test: Dunn's test, also known as the Dunn-Bonferroni test, is a non-parametric post-hoc test suitable when the assumptions of normality and homoscedasticity are violated. It's used to compare group means while controlling the familywise error rate.\n",
    "\n",
    "    Holm's Method: Holm's method is a step-down procedure that adjusts p-values for multiple comparisons. It's less conservative than Bonferroni correction and maintains familywise error control.\n",
    "\n",
    "    Fisher's Least Significant Difference (LSD): Fisher's LSD is another post-hoc test that's less stringent than Tukey's HSD. It's often used when you have a specific hypothesis about which groups might differ, as it doesn't control the familywise error rate as effectively.\n",
    "\n",
    "Example Situation:\n",
    "Suppose you conducted an experiment to compare the effectiveness of three different workout routines (A, B, and C) on cardiovascular fitness. After performing a one-way ANOVA, you found a significant difference among the means. Now, you want to determine which specific workout routines show significant differences in terms of cardiovascular fitness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bb92f-aeb1-4365-88ee-dbd3fb44756b",
   "metadata": {},
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12baf83f-c845-473a-b434-3789b9bc8f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 172.21724007247386\n",
      "p-value: 2.9860750504035665e-39\n",
      "There are significant differences between the mean weight loss of the diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "diet_A = np.array([3.2, 4.1, 2.8, 5.6, 3.9, 4.3, 2.7, 3.0, 4.7, 5.2,\n",
    "                   3.8, 4.0, 4.5, 2.9, 3.7, 4.2, 3.4, 4.6, 5.1, 3.6,\n",
    "                   2.7, 3.3, 4.8, 3.5, 4.6, 3.8, 4.2, 2.8, 4.5, 3.9,\n",
    "                   5.0, 3.2, 4.1, 3.6, 3.8, 4.9, 3.3, 4.7, 3.6, 4.0,\n",
    "                   2.9, 3.1, 4.3, 3.5, 5.2, 4.5, 3.4, 3.7, 4.6, 3.0])\n",
    "diet_B = np.array([2.5, 3.0, 3.2, 2.0, 2.8, 3.6, 2.9, 3.5, 3.1, 2.4,\n",
    "                   2.7, 2.3, 2.8, 2.6, 3.3, 2.2, 2.9, 3.1, 2.5, 2.7,\n",
    "                   3.0, 2.4, 2.8, 3.2, 2.1, 2.7, 2.9, 2.6, 2.5, 3.0,\n",
    "                   3.4, 2.8, 2.3, 2.6, 2.9, 3.0, 2.2, 3.3, 2.4, 3.1,\n",
    "                   2.6, 2.7, 2.5, 2.8, 3.0, 2.9, 2.3, 2.1, 2.7, 2.5])\n",
    "diet_C = np.array([1.8, 1.9, 2.2, 2.5, 1.7, 2.3, 1.6, 2.0, 2.4, 1.5,\n",
    "                   2.1, 1.8, 2.3, 2.2, 1.7, 2.5, 2.0, 1.9, 1.8, 2.4,\n",
    "                   2.2, 2.3, 1.6, 2.1, 2.0, 2.4, 1.9, 1.7, 2.3, 2.2,\n",
    "                   1.5, 2.0, 2.1, 2.4, 1.8, 2.3, 2.2, 1.7, 2.5, 2.0,\n",
    "                   1.9, 1.6, 2.4, 2.3, 2.1, 1.8, 1.7, 2.2, 2.0, 1.9])\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There are significant differences between the mean weight loss of the diets.\")\n",
    "else:\n",
    "    print(\"There are no significant differences between the mean weight loss of the diets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13fc81-5119-4383-b602-fe3809e942a3",
   "metadata": {},
   "source": [
    "# Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b878834a-3caf-46ca-a4bf-4343b1812c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               sum_sq    df         F    PR(>F)\n",
      "C(Software)                  4.600606   2.0  0.532542  0.589080\n",
      "C(Experience)                1.359515   1.0  0.314741  0.576279\n",
      "C(Software):C(Experience)   15.102201   2.0  1.748150  0.180369\n",
      "Residual                   362.836289  84.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "np.random.seed(0)\n",
    "software_programs = np.random.choice(['A', 'B', 'C'], size=90)\n",
    "experience_level = np.random.choice(['Novice', 'Experienced'], size=90)\n",
    "completion_times = np.random.normal(loc=10, scale=2, size=90)\n",
    "data = pd.DataFrame({'Software': software_programs,\n",
    "                     'Experience': experience_level,\n",
    "                     'Time': completion_times})\n",
    "formula = 'Time ~ C(Software) + C(Experience) + C(Software):C(Experience)'\n",
    "model = ols(formula, data).fit()\n",
    "anova_table = anova_lm(model, typ=2)\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba53188-7222-49e0-9908-b7141bab66e1",
   "metadata": {},
   "source": [
    "# Q11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07bcef2e-0076-413b-b859-5152219f3f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test results:\n",
      "T-Statistic: -1.6677351961320235\n",
      "p-value: 0.09856078338184605\n",
      "There is no significant difference in test scores between the two groups.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "np.random.seed(0)\n",
    "control_group_scores = np.random.normal(loc=70, scale=10, size=50)\n",
    "experimental_group_scores = np.random.normal(loc=75, scale=10, size=50)\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(control_group_scores, experimental_group_scores)\n",
    "\n",
    "print(\"Two-sample t-test results:\")\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in test scores between the two groups.\")\n",
    "    print(\"Performing post hoc test...\")\n",
    "    \n",
    "    all_scores = np.concatenate([control_group_scores, experimental_group_scores])\n",
    "    group_labels = np.array(['Control'] * len(control_group_scores) + ['Experimental'] * len(experimental_group_scores))\n",
    "    \n",
    "    tukey_result = pairwise_tukeyhsd(all_scores, group_labels, alpha=0.05)\n",
    "    print(tukey_result)\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the two groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db024a9-8439-4294-9612-2bff091e5950",
   "metadata": {},
   "source": [
    "# Q12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b07b865-6451-4d63-bdb5-e465afa6b8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA results:\n",
      "F-Statistic: 1.865941659055099\n",
      "p-value: 0.16089061995020176\n",
      "There is no significant difference in daily sales between the three stores.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "store_A_sales = np.random.randint(500, 1000, size=30)\n",
    "store_B_sales = np.random.randint(450, 950, size=30)\n",
    "store_C_sales = np.random.randint(400, 900, size=30)\n",
    "all_sales = np.concatenate([store_A_sales, store_B_sales, store_C_sales])\n",
    "store_labels = np.array(['Store A'] * 30 + ['Store B'] * 30 + ['Store C'] * 30)\n",
    "f_statistic, p_value = stats.f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
    "print(\"One-way ANOVA results:\")\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in daily sales between the three stores.\")\n",
    "    print(\"Performing post hoc test...\")\n",
    "    tukey_result = pairwise_tukeyhsd(all_sales, store_labels, alpha=0.05)\n",
    "    print(tukey_result)\n",
    "else:\n",
    "    print(\"There is no significant difference in daily sales between the three stores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f8ef43-c0fa-4a6f-9222-76cc3b68fdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
